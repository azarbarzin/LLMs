{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMixDpoLnbKuGdOQk8eSE82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azarbarzin/LLMs/blob/main/Lora_TinyLlama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rXBd6N7krXw",
        "outputId": "292913b1-0df9-47b6-abd4-7338bdc5b09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install transformers datasets bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline, logging\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "E_FTlRpMksGp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraFineTuner:\n",
        "  \"\"\"\n",
        "  Generic LoRA Fine-Tuning pipeline for Hugging Face causal language models.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      base_model: str,\n",
        "      dataset_name: str,\n",
        "      output_dir: str = \"./finetuned-model\",\n",
        "      num_train_epochs: int = 2,\n",
        "      learning_rate: float = 2e-4,\n",
        "      batch_size: int = 2,\n",
        "      gradient_accumulation_steps: int = 16,\n",
        "      r=64,\n",
        "      lora_alpha=16,\n",
        "      lora_dropout=0.1,\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "      # output_dir=\"./results\",\n",
        "      save_steps = 25,\n",
        "      logging_steps=1,\n",
        "      # learning_rate=2e-4,\n",
        "      weight_decay=1e-3,\n",
        "      max_grad_norm=0.3,\n",
        "      max_steps=-1,\n",
        "      warmup_ratio=0.03,\n",
        "\n",
        "\n",
        "      lr_scheduler_type=\"cosine\",\n",
        "      # num_train_epochs=2,\n",
        "      per_device_train_batch=2,\n",
        "      # gradient_accumulation_steps=16,\n",
        "      optim=\"adamw_torch\",\n",
        "      ):\n",
        "\n",
        "        self.base_model = base_model\n",
        "        self.dataset_name = dataset_name\n",
        "        self.output_dir = output_dir\n",
        "        self.num_train_epochs = num_train_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "\n",
        "\n",
        "        self.r = r\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_dropout = lora_dropout\n",
        "        self.bias = bias\n",
        "        self.task_type = task_type\n",
        "\n",
        "\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.dataset = None\n",
        "        self.trainer = None\n",
        "\n",
        "\n",
        "        self.output_dir=output_dir\n",
        "        self.save_steps = save_steps\n",
        "        self.logging_steps=logging_steps\n",
        "        self.learning_rate=learning_rate\n",
        "        self.weight_decay=weight_decay\n",
        "        self.max_grad_norm=max_grad_norm\n",
        "        self.max_steps=max_steps\n",
        "        self.lr_scheduler_type=lr_scheduler_type\n",
        "        self.num_train_epochs=num_train_epochs\n",
        "        self.per_device_train_batch=per_device_train_batch\n",
        "        self.gradient_accumulation_steps=gradient_accumulation_steps\n",
        "        self.optim=optim\n",
        "        self.warmup_ratio=warmup_ratio\n",
        "        self.warmup_steps=int(self.num_train_epochs * 0.03)\n",
        "\n",
        "\n",
        "        self.use_fp16 = False\n",
        "        self.use_bf16 = False\n",
        "        self.pin_memory = False\n",
        "        self.device_type = \"cpu\"\n",
        "\n",
        "  def detect_device_and_precision(self):\n",
        "    self.use_fp16 = False\n",
        "    self.use_bf16 = False\n",
        "    self.pin_memory = False\n",
        "    self.device_type = \"cpu\"\n",
        "\n",
        "    # CUDA First\n",
        "    if torch.cuda.is_available():\n",
        "      self.device_type = \"cuda\"\n",
        "      self.use_fp16 = True\n",
        "      self.pin_memory = True\n",
        "      return self.device_type, self.use_fp16, self.use_bf16, self.pin_memory\n",
        "\n",
        "    # TPU\n",
        "    if importlib.util.find_spec(\"torch_xla\") is not None:\n",
        "      try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        _ = xm.xla_device()\n",
        "        self.device_type = \"xla\"\n",
        "        self.use_bf16 = True\n",
        "        self.pin_memory = True\n",
        "      except Exception:\n",
        "        self.device_type = \"cpu\"\n",
        "\n",
        "    return self.device_type, self.use_fp16, self.use_bf16, self.pin_memory\n",
        "\n",
        "\n",
        "  def load_dataset(self):\n",
        "    print(f\" Loading dataset: {self.dataset_name}\")\n",
        "    self.dataset = load_dataset(self.dataset_name, split=\"train\")\n",
        "\n",
        "  def load_model_and_tokenizer(self):\n",
        "    print(f\" Loading dataset: {self.base_model}\")\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(\n",
        "        self.base_model,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    self.model.config.use_cash = False\n",
        "    self.model.config.pretraining_tp = 1\n",
        "\n",
        "    print(\" Loading tokenizer...\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.base_model, trust_remote_code=True)\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    self.tokenizer.padding_side = \"right\"\n",
        "\n",
        "\n",
        "  def quick_inference(self, prompt: str, max_length: int = 200):\n",
        "    logging.set_verbosity(logging.CRITICAL)\n",
        "    pipe = pipeline(task=\"text-generation\", model=self.model, tokenizer=self.tokenizer, max_length=max_length)\n",
        "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "    print(result[0][\"generated_text\"])\n",
        "    return result[0][\"generated_text\"]\n",
        "\n",
        "  def create_lora_config(self) -> LoraConfig:\n",
        "    print(\"Creating LoRA configuration...\")\n",
        "    return LoraConfig(\n",
        "        r = self.r,\n",
        "        lora_alpha = self.lora_alpha,\n",
        "        lora_dropout = self.lora_dropout,\n",
        "        bias = self.bias,\n",
        "        task_type = self.task_type,\n",
        "    )\n",
        "\n",
        "  def create_training_args(self) -> TrainingArguments:\n",
        "    print(\"Creating training arguments...\")\n",
        "    return TrainingArguments(\n",
        "        output_dir = self.output_dir,\n",
        "        num_train_epochs = self.num_train_epochs,\n",
        "        per_device_train_batch_size = self.batch_size,\n",
        "        gradient_accumulation_steps = self.gradient_accumulation_steps,\n",
        "        optim = self.optim,\n",
        "        save_steps = self.save_steps,\n",
        "        logging_steps = self.logging_steps,\n",
        "        learning_rate = self.learning_rate,\n",
        "        weight_decay = self.weight_decay,\n",
        "        fp16 = self.use_fp16,\n",
        "        bf16 = self.use_bfp16,\n",
        "        max_grad_norm = self.max_grad_norm,\n",
        "        max_steps = self.max_steps,\n",
        "        warmup_ratio = self.warmup_ratio,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "    )\n",
        "\n",
        "\n",
        "  def setup_trainer(self):\n",
        "    device_type, use_fp16, use_bf16, pin_memory = self.detect_device_and_precision()\n",
        "    print(f\"Detected device: {device_type.upper()} | ft16= {use_fp16}, bf16= {use_bf16}, pin_memory={pin_memory}\")\n",
        "    print(f\"Setting up SFTTrainer...\")\n",
        "\n",
        "    peft_config =LoraConfig(\n",
        "        r = self.r,\n",
        "        lora_alpha = self.lora_alpha,\n",
        "        lora_dropout = self.lora_dropout,\n",
        "        bias = self.bias,\n",
        "        task_type = self.task_type,\n",
        "\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    sft_config = SFTConfig(\n",
        "        output_dir=self.output_dir,\n",
        "        dataset_text_field=\"text\",\n",
        "        # max_seq_length=self.max_seq_length,\n",
        "        num_train_epochs=self.num_train_epochs,\n",
        "        gradient_accumulation_steps=self.gradient_accumulation_steps,\n",
        "        optim=self.optim,\n",
        "        save_steps=self.save_steps,\n",
        "        logging_steps=self.logging_steps,\n",
        "        learning_rate=self.learning_rate,\n",
        "        weight_decay=self.weight_decay,\n",
        "        fp16=self.use_fp16,\n",
        "        bf16=self.use_bf16,\n",
        "        max_grad_norm=self.max_grad_norm,\n",
        "        max_steps=self.max_steps,\n",
        "        warmup_ratio=self.warmup_ratio,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type=self.lr_scheduler_type,\n",
        "        packing=False,\n",
        "\n",
        "        warmup_steps=self.warmup_steps,\n",
        "        per_device_train_batch_size=self.per_device_train_batch,\n",
        "    )\n",
        "    self.trainer = SFTTrainer(\n",
        "        model = self.model,\n",
        "        train_dataset = self.dataset,\n",
        "        args = sft_config,\n",
        "        peft_config=peft_config\n",
        "    )\n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    print(f\"Starting fine-tuning...\")\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    self.trainer.train()\n",
        "\n",
        "\n",
        "  def save_model(self):\n",
        "    print(f\"Saving fine-tuned model to: {self.output_dir}\")\n",
        "    self.trainer.model.save_pretrained(self.output_dir)\n",
        "    self.tokenizer.save_pretrained(self.output_dir)\n",
        "\n",
        "\n",
        "  def run_full_pipeline(self, prompt: str = \"Who is Donald Trump\"):\n",
        "    # self.load_dataset()\n",
        "    # self.load_model_and_tokenizer()\n",
        "\n",
        "    # print(\"\\n=== Inference Before Fine-Tunning ===\")\n",
        "    # self.quick_inference(prompt)\n",
        "\n",
        "    # self.setup_trainer()\n",
        "    # self.train()\n",
        "    self.save_model()\n",
        "\n",
        "    print(\"\\n=== Inference After Fine-Tunning ===\")\n",
        "    self.quick_inference(prompt)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tuner = LoraFineTuner(\n",
        "        base_model=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
        "        dataset_name=\"mlabonne/guanaco-llama2-1k\",\n",
        "        output_dir=\"./llama-1.1B-chat-guanaco\",\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-4,\n",
        "        batch_size=2,\n",
        "        gradient_accumulation_steps=16,\n",
        "    )\n",
        "  tuner.run_full_pipeline()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYGLO-JcksQX",
        "outputId": "e8318aa6-c0e0-48dd-9c98-5595cbe35d84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading dataset: mlabonne/guanaco-llama2-1k\n",
            " Loading dataset: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
            " Loading tokenizer...\n",
            "\n",
            "=== Inference Before Fine-Tunning ===\n",
            "<s>[INST] Who is Trump [/INST] [REQ] [INST] [INST] [REQ] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST] [INST]\n",
            "Detected device: CUDA | ft16= True, bf16= False, pin_memory=True\n",
            "Setting up SFTTrainer...\n",
            "Starting fine-tuning...\n",
            "{'loss': 1.4787, 'grad_norm': 0.07140208780765533, 'learning_rate': 0.0, 'entropy': 1.6702846698462963, 'num_tokens': 28344.0, 'mean_token_accuracy': 0.6576919667422771, 'epoch': 0.032}\n",
            "{'loss': 1.6136, 'grad_norm': 0.08880830556154251, 'learning_rate': 0.0001, 'entropy': 1.863891914486885, 'num_tokens': 46477.0, 'mean_token_accuracy': 0.6340016275644302, 'epoch': 0.064}\n",
            "{'loss': 1.7282, 'grad_norm': 0.07710681110620499, 'learning_rate': 0.0002, 'entropy': 1.9346197620034218, 'num_tokens': 60789.0, 'mean_token_accuracy': 0.6142688691616058, 'epoch': 0.096}\n",
            "{'loss': 1.7318, 'grad_norm': 0.10110541433095932, 'learning_rate': 0.00019987165071710527, 'entropy': 1.9770398139953613, 'num_tokens': 72297.0, 'mean_token_accuracy': 0.61762710288167, 'epoch': 0.128}\n",
            "{'loss': 1.9961, 'grad_norm': 0.1140606552362442, 'learning_rate': 0.00019948693233918952, 'entropy': 2.225718103349209, 'num_tokens': 80839.0, 'mean_token_accuracy': 0.5679814927279949, 'epoch': 0.16}\n",
            "{'loss': 2.2193, 'grad_norm': 0.15714524686336517, 'learning_rate': 0.00019884683243281116, 'entropy': 2.3516008108854294, 'num_tokens': 86557.0, 'mean_token_accuracy': 0.5446010455489159, 'epoch': 0.192}\n",
            "{'loss': 2.774, 'grad_norm': 0.3003562092781067, 'learning_rate': 0.00019795299412524945, 'entropy': 2.8218239098787308, 'num_tokens': 89298.0, 'mean_token_accuracy': 0.4428759440779686, 'epoch': 0.224}\n",
            "{'loss': 1.6234, 'grad_norm': 0.09641829878091812, 'learning_rate': 0.00019680771188662044, 'entropy': 1.7904065996408463, 'num_tokens': 120394.0, 'mean_token_accuracy': 0.6346570290625095, 'epoch': 0.256}\n",
            "{'loss': 1.6652, 'grad_norm': 0.11372871696949005, 'learning_rate': 0.00019541392564000488, 'entropy': 1.8172316327691078, 'num_tokens': 139965.0, 'mean_token_accuracy': 0.6231601536273956, 'epoch': 0.288}\n",
            "{'loss': 1.7704, 'grad_norm': 0.10585606843233109, 'learning_rate': 0.00019377521321470805, 'entropy': 1.9483338743448257, 'num_tokens': 154902.0, 'mean_token_accuracy': 0.6049654260277748, 'epoch': 0.32}\n",
            "{'loss': 1.8055, 'grad_norm': 0.13211461901664734, 'learning_rate': 0.00019189578116202307, 'entropy': 1.9669266194105148, 'num_tokens': 166806.0, 'mean_token_accuracy': 0.602151770144701, 'epoch': 0.352}\n",
            "{'loss': 2.008, 'grad_norm': 0.17072048783302307, 'learning_rate': 0.00018978045395707418, 'entropy': 2.120245486497879, 'num_tokens': 175833.0, 'mean_token_accuracy': 0.5703165866434574, 'epoch': 0.384}\n",
            "{'loss': 2.0123, 'grad_norm': 0.23746046423912048, 'learning_rate': 0.00018743466161445823, 'entropy': 2.1375026181340218, 'num_tokens': 181886.0, 'mean_token_accuracy': 0.5759047120809555, 'epoch': 0.416}\n",
            "{'loss': 2.3959, 'grad_norm': 0.42054152488708496, 'learning_rate': 0.00018486442574947511, 'entropy': 2.494337484240532, 'num_tokens': 185028.0, 'mean_token_accuracy': 0.5005356781184673, 'epoch': 0.448}\n",
            "{'loss': 1.435, 'grad_norm': 0.11496714502573013, 'learning_rate': 0.00018207634412072764, 'entropy': 1.5162097960710526, 'num_tokens': 216824.0, 'mean_token_accuracy': 0.6719526834785938, 'epoch': 0.48}\n",
            "{'loss': 1.6506, 'grad_norm': 0.10081002116203308, 'learning_rate': 0.00017907757369376985, 'entropy': 1.7475890666246414, 'num_tokens': 239125.0, 'mean_token_accuracy': 0.6231626011431217, 'epoch': 0.512}\n",
            "{'loss': 1.6474, 'grad_norm': 0.10114241391420364, 'learning_rate': 0.0001758758122692791, 'entropy': 1.7834031581878662, 'num_tokens': 255420.0, 'mean_token_accuracy': 0.6317600142210722, 'epoch': 0.544}\n",
            "{'loss': 1.6052, 'grad_norm': 0.10707256942987442, 'learning_rate': 0.000172479278722912, 'entropy': 1.7363401725888252, 'num_tokens': 268195.0, 'mean_token_accuracy': 0.6293208003044128, 'epoch': 0.576}\n",
            "{'loss': 1.7565, 'grad_norm': 0.1341768056154251, 'learning_rate': 0.00016889669190756868, 'entropy': 1.8072988912463188, 'num_tokens': 278003.0, 'mean_token_accuracy': 0.6050623115152121, 'epoch': 0.608}\n",
            "{'loss': 1.9273, 'grad_norm': 0.16643905639648438, 'learning_rate': 0.00016513724827222227, 'entropy': 2.0132516473531723, 'num_tokens': 284986.0, 'mean_token_accuracy': 0.569488562643528, 'epoch': 0.64}\n",
            "{'loss': 2.1975, 'grad_norm': 0.3375173509120941, 'learning_rate': 0.0001612105982547663, 'entropy': 2.2090402022004128, 'num_tokens': 288643.0, 'mean_token_accuracy': 0.5358995459973812, 'epoch': 0.672}\n",
            "{'loss': 1.3391, 'grad_norm': 0.08005575835704803, 'learning_rate': 0.00015712682150947923, 'entropy': 1.3865230530500412, 'num_tokens': 317716.0, 'mean_token_accuracy': 0.6855239421129227, 'epoch': 0.704}\n",
            "{'loss': 1.6346, 'grad_norm': 0.08951587975025177, 'learning_rate': 0.00015289640103269625, 'entropy': 1.672801986336708, 'num_tokens': 336740.0, 'mean_token_accuracy': 0.6315281707793474, 'epoch': 0.736}\n",
            "{'loss': 1.6257, 'grad_norm': 0.11226458102464676, 'learning_rate': 0.00014853019625310813, 'entropy': 1.714518018066883, 'num_tokens': 351226.0, 'mean_token_accuracy': 0.6237681731581688, 'epoch': 0.768}\n",
            "{'loss': 1.5429, 'grad_norm': 0.1345045566558838, 'learning_rate': 0.00014403941515576344, 'entropy': 1.6301851272583008, 'num_tokens': 362337.0, 'mean_token_accuracy': 0.639994278550148, 'epoch': 0.8}\n",
            "{'loss': 1.772, 'grad_norm': 0.17367538809776306, 'learning_rate': 0.00013943558551133186, 'entropy': 1.8693792298436165, 'num_tokens': 370536.0, 'mean_token_accuracy': 0.6114503815770149, 'epoch': 0.832}\n",
            "{'loss': 2.0386, 'grad_norm': 0.23415988683700562, 'learning_rate': 0.00013473052528448201, 'entropy': 2.027580551803112, 'num_tokens': 376228.0, 'mean_token_accuracy': 0.5659214854240417, 'epoch': 0.864}\n",
            "{'loss': 2.4541, 'grad_norm': 0.49281197786331177, 'learning_rate': 0.00012993631229733582, 'entropy': 2.4203634336590767, 'num_tokens': 379024.0, 'mean_token_accuracy': 0.49520063027739525, 'epoch': 0.896}\n",
            "{'loss': 1.4247, 'grad_norm': 0.10436277091503143, 'learning_rate': 0.00012506525322587207, 'entropy': 1.5066274777054787, 'num_tokens': 402800.0, 'mean_token_accuracy': 0.6597377024590969, 'epoch': 0.928}\n",
            "{'loss': 1.7307, 'grad_norm': 0.142074316740036, 'learning_rate': 0.00012012985200886602, 'entropy': 1.7968680933117867, 'num_tokens': 416185.0, 'mean_token_accuracy': 0.6062076240777969, 'epoch': 0.96}\n",
            "{'loss': 1.8553, 'grad_norm': 0.3165215849876404, 'learning_rate': 0.00011514277775045768, 'entropy': 2.0430251508951187, 'num_tokens': 421908.0, 'mean_token_accuracy': 0.5746818725019693, 'epoch': 0.992}\n",
            "{'loss': 2.4156, 'grad_norm': 1.0741117000579834, 'learning_rate': 0.00011011683219874323, 'entropy': 2.5283743143081665, 'num_tokens': 422442.0, 'mean_token_accuracy': 0.4964093342423439, 'epoch': 1.0}\n",
            "{'loss': 1.2787, 'grad_norm': 0.0972975492477417, 'learning_rate': 0.00010506491688387127, 'entropy': 1.3365669250488281, 'num_tokens': 453457.0, 'mean_token_accuracy': 0.6969983726739883, 'epoch': 1.032}\n",
            "{'loss': 1.4924, 'grad_norm': 0.11144404113292694, 'learning_rate': 0.0001, 'entropy': 1.5449724718928337, 'num_tokens': 473301.0, 'mean_token_accuracy': 0.648176409304142, 'epoch': 1.064}\n",
            "{'loss': 1.7193, 'grad_norm': 0.14170876145362854, 'learning_rate': 9.493508311612874e-05, 'entropy': 1.7672533243894577, 'num_tokens': 488290.0, 'mean_token_accuracy': 0.6198441497981548, 'epoch': 1.096}\n",
            "{'loss': 1.5795, 'grad_norm': 0.13795985281467438, 'learning_rate': 8.98831678012568e-05, 'entropy': 1.7082825750112534, 'num_tokens': 500535.0, 'mean_token_accuracy': 0.6414535008370876, 'epoch': 1.1280000000000001}\n",
            "{'loss': 1.7123, 'grad_norm': 0.1783287227153778, 'learning_rate': 8.485722224954237e-05, 'entropy': 1.7949056401848793, 'num_tokens': 509682.0, 'mean_token_accuracy': 0.6034037657082081, 'epoch': 1.16}\n",
            "{'loss': 1.9643, 'grad_norm': 0.2329665869474411, 'learning_rate': 7.987014799113397e-05, 'entropy': 2.0314842611551285, 'num_tokens': 515426.0, 'mean_token_accuracy': 0.5768566615879536, 'epoch': 1.192}\n",
            "{'loss': 2.0851, 'grad_norm': 0.446133553981781, 'learning_rate': 7.493474677412794e-05, 'entropy': 2.1291085183620453, 'num_tokens': 518250.0, 'mean_token_accuracy': 0.5617818273603916, 'epoch': 1.224}\n",
            "{'loss': 1.4345, 'grad_norm': 0.08645865321159363, 'learning_rate': 7.006368770266421e-05, 'entropy': 1.4646981209516525, 'num_tokens': 547889.0, 'mean_token_accuracy': 0.6641581393778324, 'epoch': 1.256}\n",
            "{'loss': 1.5694, 'grad_norm': 0.0967201218008995, 'learning_rate': 6.526947471551798e-05, 'entropy': 1.6505466923117638, 'num_tokens': 568965.0, 'mean_token_accuracy': 0.6386693008244038, 'epoch': 1.288}\n",
            "{'loss': 1.5766, 'grad_norm': 0.1148705706000328, 'learning_rate': 6.0564414488668165e-05, 'entropy': 1.691108986735344, 'num_tokens': 584389.0, 'mean_token_accuracy': 0.6418697014451027, 'epoch': 1.32}\n",
            "{'loss': 1.5056, 'grad_norm': 0.1216353252530098, 'learning_rate': 5.596058484423656e-05, 'entropy': 1.6149452812969685, 'num_tokens': 596599.0, 'mean_token_accuracy': 0.6491357497870922, 'epoch': 1.3519999999999999}\n",
            "{'loss': 1.634, 'grad_norm': 0.13798806071281433, 'learning_rate': 5.146980374689192e-05, 'entropy': 1.7036553546786308, 'num_tokens': 606278.0, 'mean_token_accuracy': 0.6345937103033066, 'epoch': 1.384}\n",
            "{'loss': 1.8362, 'grad_norm': 0.18865174055099487, 'learning_rate': 4.710359896730379e-05, 'entropy': 1.89817563444376, 'num_tokens': 613115.0, 'mean_token_accuracy': 0.5974616780877113, 'epoch': 1.416}\n",
            "{'loss': 2.0951, 'grad_norm': 0.38477379083633423, 'learning_rate': 4.287317849052075e-05, 'entropy': 2.093184322118759, 'num_tokens': 616207.0, 'mean_token_accuracy': 0.5542288646101952, 'epoch': 1.448}\n",
            "{'loss': 1.4783, 'grad_norm': 0.094937264919281, 'learning_rate': 3.878940174523371e-05, 'entropy': 1.4949988424777985, 'num_tokens': 645405.0, 'mean_token_accuracy': 0.6562827341258526, 'epoch': 1.48}\n",
            "{'loss': 1.4522, 'grad_norm': 0.10209304094314575, 'learning_rate': 3.4862751727777797e-05, 'entropy': 1.5439051166176796, 'num_tokens': 664887.0, 'mean_token_accuracy': 0.6599568873643875, 'epoch': 1.512}\n",
            "{'loss': 1.5859, 'grad_norm': 0.1192535012960434, 'learning_rate': 3.110330809243134e-05, 'entropy': 1.681807205080986, 'num_tokens': 680439.0, 'mean_token_accuracy': 0.6370807439088821, 'epoch': 1.544}\n",
            "{'loss': 1.5058, 'grad_norm': 0.11461344361305237, 'learning_rate': 2.7520721277088024e-05, 'entropy': 1.6024084016680717, 'num_tokens': 692431.0, 'mean_token_accuracy': 0.6475663296878338, 'epoch': 1.576}\n",
            "{'loss': 1.7995, 'grad_norm': 0.17064037919044495, 'learning_rate': 2.4124187730720917e-05, 'entropy': 1.849043883383274, 'num_tokens': 700886.0, 'mean_token_accuracy': 0.6047971174120903, 'epoch': 1.608}\n",
            "{'loss': 1.8841, 'grad_norm': 0.19423246383666992, 'learning_rate': 2.092242630623016e-05, 'entropy': 1.9417852386832237, 'num_tokens': 706771.0, 'mean_token_accuracy': 0.5865906868129969, 'epoch': 1.6400000000000001}\n",
            "{'loss': 2.0757, 'grad_norm': 0.4015273451805115, 'learning_rate': 1.7923655879272393e-05, 'entropy': 2.122356154024601, 'num_tokens': 709789.0, 'mean_token_accuracy': 0.5599913690239191, 'epoch': 1.6720000000000002}\n",
            "{'loss': 1.3751, 'grad_norm': 0.09127604961395264, 'learning_rate': 1.5135574250524897e-05, 'entropy': 1.4718182384967804, 'num_tokens': 739771.0, 'mean_token_accuracy': 0.6768770590424538, 'epoch': 1.704}\n",
            "{'loss': 1.4087, 'grad_norm': 0.10039129853248596, 'learning_rate': 1.2565338385541792e-05, 'entropy': 1.4996165558695793, 'num_tokens': 757943.0, 'mean_token_accuracy': 0.6661812029778957, 'epoch': 1.736}\n",
            "{'loss': 1.632, 'grad_norm': 0.10914601385593414, 'learning_rate': 1.0219546042925843e-05, 'entropy': 1.6881647929549217, 'num_tokens': 771890.0, 'mean_token_accuracy': 0.6168886721134186, 'epoch': 1.768}\n",
            "{'loss': 1.6643, 'grad_norm': 0.12350746244192123, 'learning_rate': 8.10421883797694e-06, 'entropy': 1.7862195521593094, 'num_tokens': 783170.0, 'mean_token_accuracy': 0.6184261161834002, 'epoch': 1.8}\n",
            "{'loss': 1.5373, 'grad_norm': 0.1391659379005432, 'learning_rate': 6.22478678529197e-06, 'entropy': 1.678990676999092, 'num_tokens': 791508.0, 'mean_token_accuracy': 0.639262743294239, 'epoch': 1.8319999999999999}\n",
            "{'loss': 1.7235, 'grad_norm': 0.2064279168844223, 'learning_rate': 4.586074359995119e-06, 'entropy': 1.838672399520874, 'num_tokens': 797235.0, 'mean_token_accuracy': 0.6143165063112974, 'epoch': 1.8639999999999999}\n",
            "{'loss': 2.0919, 'grad_norm': 0.3963358700275421, 'learning_rate': 3.1922881133795825e-06, 'entropy': 2.1323827505111694, 'num_tokens': 800014.0, 'mean_token_accuracy': 0.5482338014990091, 'epoch': 1.896}\n",
            "{'loss': 1.465, 'grad_norm': 0.09176455438137054, 'learning_rate': 2.0470058747505516e-06, 'entropy': 1.5601619258522987, 'num_tokens': 824935.0, 'mean_token_accuracy': 0.6528045870363712, 'epoch': 1.928}\n",
            "{'loss': 1.4534, 'grad_norm': 0.1028984934091568, 'learning_rate': 1.1531675671888619e-06, 'entropy': 1.5874841660261154, 'num_tokens': 837891.0, 'mean_token_accuracy': 0.6487866826355457, 'epoch': 1.96}\n",
            "{'loss': 1.7865, 'grad_norm': 0.16581875085830688, 'learning_rate': 5.130676608104845e-07, 'entropy': 1.8520630598068237, 'num_tokens': 844277.0, 'mean_token_accuracy': 0.5935507118701935, 'epoch': 1.992}\n",
            "{'loss': 2.2338, 'grad_norm': 0.9056248664855957, 'learning_rate': 1.2834928289472416e-07, 'entropy': 2.2005929350852966, 'num_tokens': 844884.0, 'mean_token_accuracy': 0.515744723379612, 'epoch': 2.0}\n",
            "{'train_runtime': 533.3612, 'train_samples_per_second': 3.75, 'train_steps_per_second': 0.12, 'train_loss': 1.757990475744009, 'epoch': 2.0}\n",
            "Saving fine-tuned model to: ./llama-1.1B-chat-guanaco\n",
            "\n",
            "=== Inference After Fine-Tunning ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Who is Trump [/INST] Donald [ [ [ [ [ [ [ [ [\n",
            "\n",
            "\n",
            "            [ [ [ [\n",
            "\n",
            "        [ [ [ [ [ [S [ [ [\n",
            "\n",
            "    [\n",
            "\n",
            "    [\n",
            "  [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [[ [\n",
            "\n",
            "                [ [2079999.\n",
            " * [ [\n",
            "            [ [ [ [ [ [ [ [ [ [ [ ] [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [ [\n",
            "\n",
            "\n",
            "        [ [ [ [ [ [ [ [ [\n",
            "  [ [ [ [ [007, and [ [ [ [\n",
            "        [ [ [ [ [ [ [\n",
            "                                    [ [ [ [\n",
            "\n",
            "    private [ [ [ [\n",
            "   [\n",
            "\n",
            "\n",
            "\n",
            "    [ [ [ [ [ [002\n",
            "     [ [ [  [ [\n"
          ]
        }
      ]
    }
  ]
}